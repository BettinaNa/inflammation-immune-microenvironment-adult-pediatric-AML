---
title: "ScDblFinder filtering with recoverDoublets - `r params$sample_name`"
author: 'User ID: `r Sys.getenv("USER")`'
date: "`r Sys.Date()`"
output:
  html_document:
    fig_height: 7
    fig_width: 9
    keep_md: yes
    toc: yes
    df_print: paged
params:
  library_path_file: "/gpfs/data/aifantislab/home/nadorb01/projects/ad_ped_AML_v5/data/metadata/libraries_wo_hashes.txt" # path to the directory with the outs file
  out_path: "/gpfs/data/aifantislab/home/nadorb01/projects/ad_ped_AML_v5/data/scDblFinder_non_hashed/" # output path where the outputs will be stored
  log_file: NULL
---

```{r setup, include=FALSE}
attach(params) 
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.path = file.path(out_path))
```

```{r set_up_env}
library(scDblFinder)
library(Seurat)
library(RColorBrewer)
library(ggplot2)
library(scater)
library(future)
library(stringr)
library(dplyr)

# # evaluate Seurat R expressions asynchronously when possible (such as ScaleData) using future package
plan("multiprocess", workers = 4)
options(future.globals.maxSize = 64000 * 1024^2) # for 64 Gb RAM

#create out_path
dir.create(out_path, recursive=TRUE)


dirs <- read.delim2(library_path_file, sep=" ", stringsAsFactors=FALSE)
dirs <- dirs$x

```

# Read raw seurat_objs 
We need to load all the raw objects, run dimensionality reductions and convert all libraries to single cell experiments.
We are using the following non-hashed objects here: 
`r dirs`

```{r read_seurat_objs_dim_reductions_sce_conversion}

object_list <- list()

# read seurat_objects
for (i in seq_along(dirs)){
    object_list[[i]] <- readRDS(dirs[i])
    DefaultAssay(object = object_list[[i]]) <- "RNA"

    # run dim_reductions on each object
    set.seed(1234)
    object_list[[i]] <- object_list[[i]] %>%
                    NormalizeData() %>%
                    FindVariableFeatures(selection.method = "vst", nfeatures = 2000) %>%
                    ScaleData()

    object_list[[i]] <- RunPCA(object_list[[i]], features = VariableFeatures(object = object_list[[i]]), npcs=30) %>%
                    RunUMAP(dims = 1:20, n.neighbors=20L, reduction.name = "umap", reduction.key = "UMAP_", verbose = FALSE)

    object_list[[i]] <- as.SingleCellExperiment(object_list[[i]])
}

```

# Save sce objects with dim reductions
Object_list is saved to: `r paste0(out_path, "object_list_sce.rds")`.

```{r save_obj_list}
saveRDS(object_list, paste0(out_path, "object_list_sce.rds"))
```

# Run scDblFinder on all objects
We are running the scDblFinder function from the scDblFinder package to identify cells that are doublets based on 
cluster-based generation of artifical doublets. 
For each library the scDblFinder output, the cells that need to be removed and the single cell experiment object
are saved to a directory with the same name as the orig.ident of each library. 
Finally, we are saving a combined file with all the cells that need to be filtered based on this analysis to 
`r paste0(out_path, "hashed_doublets_to_filter_combined.csv")`. The data is not quality filtered at this point.

```{r run_recoverDoublets}

for (i in seq_along(dirs)){

    # create library dir
    sample_name <- gsub("Create-", "", str_split_fixed(dirs[i],"/",12)[,11])
    temp_path <- paste0(out_path,"/scDblFinder-", sample_name, "/")
    dir.create(temp_path)
    
    print(sample_name)
    #run scDoublet Finder
    bp <- BiocParallel::MulticoreParam(4, RNGseed=1234)
    BiocParallel::bpstart(bp)
    set.seed(1234)
    object_list[[i]]  <- scDblFinder(object_list[[i]], trajectoryMode=TRUE, BPPARAM= bp)
    BiocParallel::bpstop(bp)


    print("doublets detected:")
    print(table(object_list[[i]]$scDblFinder.class))

    print("This is % of full library:")
    print(table(object_list[[i]]$scDblFinder.class)[2]/(table(object_list[[i]]$scDblFinder.class)[1]+table(object_list[[i]]$scDblFinder.class)[2])*100)

    hashed.doublets <- object_list[[i]]@colData[,str_detect(colnames(object_list[[i]]@colData),"scDblFinder")] %>% as.data.frame() %>% tibble::rownames_to_column("cell")
    write.csv(hashed.doublets, paste0(temp_path, "hashed_doublets_", sample_name,".csv"))
    
    # filter to cells that we need to filter from final object
    hashed.doublets.filter <- subset(hashed.doublets, hashed.doublets$scDblFinder.class == "doublet")


    print("Doublet predictions")
    print(gridExtra::grid.arrange(
        plotUMAP(object_list[[i]], colour_by="scDblFinder.cluster", point_size = 0.1) + ggtitle("Clusters") ,
        plotUMAP(object_list[[i]], colour_by="scDblFinder.score", point_size = 0.1) + ggtitle("Doublet score") ,
        plotUMAP(object_list[[i]], colour_by="scDblFinder.class", point_size = 0.1) + ggtitle("Predicted doublets") ,
        ncol=2        
    ))


    pdf(paste0(temp_path, "recover_doublet_predictions.pdf"), onefile=FALSE, useDingbats=F, height=12, width=12)
    print(gridExtra::grid.arrange(
        plotUMAP(object_list[[i]], colour_by="scDblFinder.cluster", point_size = 0.1) + ggtitle("Clusters") ,
        plotUMAP(object_list[[i]], colour_by="scDblFinder.score", point_size = 0.1) + ggtitle("Doublet score") ,
        plotUMAP(object_list[[i]], colour_by="scDblFinder.class", point_size = 0.1) + ggtitle("Predicted doublets") ,
        ncol=2        
    ))
    dev.off()


    saveRDS(object_list[[i]], paste0(temp_path, "sce_obj_", sample_name,".rds") )

    if(i == 1){
        hashed.doublets.combined <- hashed.doublets.filter
    } else {
        hashed.doublets.combined <- rbind(hashed.doublets.combined, hashed.doublets.filter)
    }
}

write.csv(hashed.doublets.combined, paste0(out_path, "non_hashed_doublets_to_filter_combined.csv"))


```
